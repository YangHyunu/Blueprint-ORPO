"""
KRSAT Llama 3.1 Inference Script
ìˆ˜ëŠ¥ ë¬¸ì œ í’€ì´ë¥¼ ìœ„í•œ Llama 3.1 ëª¨ë¸ ì¶”ë¡  ìŠ¤í¬ë¦½íŠ¸

âš ï¸ ì‹œìŠ¤í…œ ìš”êµ¬ì‚¬í•­:
- ìµœì†Œ 8GB ì´ìƒì˜ ë””ìŠ¤í¬ ì—¬ìœ  ê³µê°„ (ëª¨ë¸ ë‹¤ìš´ë¡œë“œìš©)
- ìµœì†Œ 16GB RAM ê¶Œì¥
- GPU ì‚¬ìš© ì‹œ: CUDA ì§€ì› GPU ê¶Œì¥ 
"""


import re
from ast import literal_eval
import pandas as pd
import random
from tqdm import tqdm
from huggingface_hub import hf_hub_download
from llama_cpp import Llama


# í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ì •ì˜
PROMPT_NO_QUESTION_PLUS = """ì§€ë¬¸:
{paragraph}

ì§ˆë¬¸:
{question}

ì„ íƒì§€:
{choices}

1, 2, 3, 4, 5 ì¤‘ì— í•˜ë‚˜ë¥¼ ì •ë‹µìœ¼ë¡œ ê³ ë¥´ì„¸ìš”.
ì •ë‹µ:"""

PROMPT_QUESTION_PLUS = """ì§€ë¬¸:
{paragraph}

ì§ˆë¬¸:
{question}

<ë³´ê¸°>:
{question_plus}

ì„ íƒì§€:
{choices}

1, 2, 3, 4, 5 ì¤‘ì— í•˜ë‚˜ë¥¼ ì •ë‹µìœ¼ë¡œ ê³ ë¥´ì„¸ìš”.
ì •ë‹µ:"""


def load_and_prepare_test_data(test_csv_path):
    """í…ŒìŠ¤íŠ¸ ë°ì´í„°ë¥¼ ë¡œë“œí•˜ê³  ì „ì²˜ë¦¬í•©ë‹ˆë‹¤."""
    print(f"í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¡œë”© ì¤‘: {test_csv_path}")
    test_df = pd.read_csv(test_csv_path)
    
    # Flatten the JSON dataset
    records = []
    for _, row in test_df.iterrows():
        problems = literal_eval(row['problems'])
        record = {
            'id': row['id'],
            'paragraph': row['paragraph'],
            'question': problems['question'],
            'choices': problems['choices'],
            'answer': problems.get('answer', None),
            "question_plus": problems.get('question_plus', None),
        }
        # Include 'question_plus' if it exists
        if 'question_plus' in problems:
            record['question_plus'] = problems['question_plus']
        records.append(record)
    
    # Convert to DataFrame
    test_df = pd.DataFrame(records)
    print(f"{len(test_df)}ê°œì˜ í…ŒìŠ¤íŠ¸ ìƒ˜í”Œ ë¡œë“œ ì™„ë£Œ")
    return test_df


def prepare_test_dataset(test_df):
    """í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ì„ ëª¨ë¸ ì…ë ¥ í˜•ì‹ìœ¼ë¡œ ë³€í™˜í•©ë‹ˆë‹¤."""
    print("í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ ë³€í™˜ ì¤‘...")
    test_dataset = []
    
    for i, row in test_df.iterrows():
        choices_string = "\n".join([f"{idx + 1} - {choice}" for idx, choice in enumerate(row["choices"])])
        len_choices = len(row["choices"])

        # <ë³´ê¸°>ê°€ ìˆì„ ë•Œ
        if row["question_plus"]:
            user_message = PROMPT_QUESTION_PLUS.format(
                paragraph=row["paragraph"],
                question=row["question"],
                question_plus=row["question_plus"],
                choices=choices_string,
            )
        # <ë³´ê¸°>ê°€ ì—†ì„ ë•Œ
        else:
            user_message = PROMPT_NO_QUESTION_PLUS.format(
                paragraph=row["paragraph"],
                question=row["question"],
                choices=choices_string,
            )

        test_dataset.append(
            {
                "id": row["id"],
                "messages": [
                    {"role": "system", "content": "ì§€ë¬¸ì„ ì½ê³  ì§ˆë¬¸ì˜ ë‹µì„ êµ¬í•˜ì„¸ìš”."},
                    {"role": "user", "content": user_message},
                ],
                "label": row["answer"],
                "len_choices": len_choices,
            }
        )
    
    print(f"{len(test_dataset)}ê°œì˜ ìƒ˜í”Œ ë³€í™˜ ì™„ë£Œ")
    return test_dataset


def download_model(repo_id, filename):
    """HuggingFace Hubì—ì„œ ëª¨ë¸ì„ ë‹¤ìš´ë¡œë“œí•©ë‹ˆë‹¤."""
    print(f"ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ì¤‘: {repo_id}/{filename}")
    model_path = hf_hub_download(
        repo_id=repo_id,
        filename=filename,
    )
    print(f"ë‹¤ìš´ë¡œë“œ ì™„ë£Œ: {model_path}")
    return model_path


def load_llm_model(model_path, n_ctx=4096, n_gpu_layers=-1):
    """Llama ëª¨ë¸ì„ ë¡œë“œí•©ë‹ˆë‹¤."""
    print(f" ëª¨ë¸ ë¡œë”© ì¤‘...")
    llm = Llama(
        model_path=model_path,
        n_ctx=n_ctx,
        n_gpu_layers=n_gpu_layers,
        verbose=False,
    )
    print(f"ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")
    return llm


def run_inference(llm, test_dataset, max_tokens=10, temperature=0.1, top_p=0.9):
    """í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ì— ëŒ€í•´ ì¶”ë¡ ì„ ì‹¤í–‰í•©ë‹ˆë‹¤."""
    print(f"ğŸ”® ì¶”ë¡  ì‹œì‘ (ì´ {len(test_dataset)}ê°œ ìƒ˜í”Œ)...")
    infer_results = []
    
    for data in tqdm(test_dataset, desc="Inference"):
        _id = data["id"]
        messages = data["messages"]

        response = llm.create_chat_completion(
            messages=messages,
            max_tokens=max_tokens,
            temperature=temperature,
            top_p=top_p,
        )

        # ì‘ë‹µ í…ìŠ¤íŠ¸ ì¶”ì¶œ
        out_text = response['choices'][0]['message']['content'].strip()

        # 1~5 ì‚¬ì´ì˜ ìˆ«ì ì¶”ì¶œ (ì •ë‹µ í˜•ì‹ì´ "1", "1.", "ì •ë‹µ: 1" ë“±ìœ¼ë¡œ ë‚˜ì˜¬ ìˆ˜ ìˆìŒ)
        match = re.search(r'[1-5]', out_text)
        if match:
            predict_value = match.group(0)
        else:
            # ìˆ«ìë¥¼ ì°¾ì§€ ëª»í•œ ê²½ìš° (ì°ì–´)
            print(f"ê²½ê³ : ìƒ˜í”Œ ID {_id}ì—ì„œ ìœ íš¨í•œ ë‹µë³€ì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ì¶œë ¥: '{out_text}'")
            
            predict_value = str(random.randint(1, min(data["len_choices"], 5)))

        infer_results.append({"id": _id, "answer": predict_value})
    
    print(f"ì¶”ë¡  ì™„ë£Œ")
    return infer_results


def save_results(infer_results, output_path):
    """ì¶”ë¡  ê²°ê³¼ë¥¼ CSV íŒŒì¼ë¡œ ì €ì¥í•©ë‹ˆë‹¤."""
    print(f"ê²°ê³¼ ì €ì¥ ì¤‘: {output_path}")
    pred_df = pd.DataFrame(infer_results)
    pred_df.to_csv(output_path, index=False)
    print(f"ì €ì¥ ì™„ë£Œ")
    print(f"\nê²°ê³¼ ë¯¸ë¦¬ë³´ê¸°:")
    print(pred_df.head())
    return pred_df


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    # === ì„¤ì • ===
    TEST_CSV_PATH = './data/test.csv'  # í…ŒìŠ¤íŠ¸ ë°ì´í„° ê²½ë¡œ
    OUTPUT_PATH = 'krsat_predictions.csv'  # ì¶œë ¥ íŒŒì¼ ê²½ë¡œ
    
    MODEL_REPO_ID = "Hyunwoo98/Llama-3.1-8B-KRSAT-GGUF"
    MODEL_FILENAME = "Meta-Llama-3.1-8B-Instruct.Q8_0.gguf"
    
    # === ë°ì´í„° ë¡œë“œ ë° ì „ì²˜ë¦¬ ===
    test_df = load_and_prepare_test_data(TEST_CSV_PATH)
    test_dataset = prepare_test_dataset(test_df)
    
    # === ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ë° ë¡œë“œ ===
    model_path = download_model(MODEL_REPO_ID, MODEL_FILENAME)
    llm = load_llm_model(model_path)
    
    # === ìƒ˜í”Œ í…ŒìŠ¤íŠ¸ (ì„ íƒì‚¬í•­) ===
    print("\n ìƒ˜í”Œ í…ŒìŠ¤íŠ¸ ì‹¤í–‰...")
    sample_data = test_dataset[0]
    print(f"ì§ˆë¬¸ ë¯¸ë¦¬ë³´ê¸°: {sample_data['messages'][1]['content'][:]}...")
    
    response = llm.create_chat_completion(
        messages=sample_data['messages'],
        max_tokens=10,
        temperature=0.2,
    )
    
    print("\n--- ëª¨ë¸ ì‘ë‹µ ---")
    print(response['choices'][0]['message']['content'])
    print("---------------\n")
    
    # === ì „ì²´ ì¶”ë¡  ì‹¤í–‰ ===
    infer_results = run_inference(llm, test_dataset)
    
    # === ê²°ê³¼ ì €ì¥ ===
    pred_df = save_results(infer_results, OUTPUT_PATH)
    
    print(f"\nëª¨ë“  ì‘ì—…ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")
    print(f"ì´ {len(infer_results)}ê°œì˜ ì˜ˆì¸¡ ê²°ê³¼ê°€ '{OUTPUT_PATH}'ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")


if __name__ == "__main__":
    main()
