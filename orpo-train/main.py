import unsloth 
from unsloth import PatchDPOTrainer

import hydra
from omegaconf import DictConfig
import wandb
import os
from dotenv import load_dotenv
from trl import ORPOConfig, ORPOTrainer

# ìš°ë¦¬ê°€ ë§Œë“  ëª¨ë“ˆ ì„í¬íŠ¸
from src.model import load_model_and_tokenizer
from src.data import load_and_format_dataset

load_dotenv()

@hydra.main(version_base=None, config_path="configs", config_name="config")
def main(cfg: DictConfig):
    # 1. ì‹¤í—˜ ì¶”ì  ì‹œì‘ (WandB)
    wandb.init(project=cfg.wandb.project, name=cfg.wandb.run_name)
    
    # Unsloth íŒ¨ì¹˜ ì ìš© (ë©”ëª¨ë¦¬ ìµœì í™”)
    PatchDPOTrainer()

    # 2. ëª¨ë¸ ì¤€ë¹„
    model, tokenizer = load_model_and_tokenizer(cfg)

    # 3. ë°ì´í„° ì¤€ë¹„ (ìœ ì—°í•œ ë¡œë”©)
    dataset = load_and_format_dataset(cfg, tokenizer)
    
    # ë°ì´í„° í™•ì¸ìš© ì¶œë ¥
    print("="*30)
    print("Example Data:")
    print(dataset[0]['prompt'][:300]) # ì•ë¶€ë¶„ë§Œ ì¶œë ¥ í™•ì¸
    print("="*30)

    # 4. í•™ìŠµ ì„¤ì • (ORPO)
    orpo_args = ORPOConfig(
        per_device_train_batch_size=cfg.training.batch_size,
        gradient_accumulation_steps=cfg.training.gradient_accumulation_steps,
        learning_rate=cfg.training.learning_rate,
        max_length=cfg.training.max_seq_length,
        max_prompt_length=cfg.training.max_seq_length // 2,
        max_completion_length=cfg.training.max_seq_length // 2,
        num_train_epochs=cfg.training.num_train_epochs,
        logging_steps=cfg.training.logging_steps,
        output_dir=cfg.training.output_dir,
        optim=cfg.training.optim,
        warmup_ratio=cfg.training.warmup_ratio,
        lr_scheduler_type=cfg.training.lr_scheduler_type,
        beta= cfg.training.beta,
        report_to="wandb",
        remove_unused_columns=False, # ë§¤í•‘ëœ ë°ì´í„° ë³´ì¡´ì„ ìœ„í•´ False ì¶”ì²œ
    )

    trainer = ORPOTrainer(
        model=model,
        train_dataset=dataset,
        tokenizer=tokenizer,
        args=orpo_args,
    )

    # 5. í•™ìŠµ ì‹œì‘
    print("ğŸš€ Starting Training...")
    trainer.train()

    # 6. ì €ì¥
    final_path = os.path.join(cfg.training.output_dir, "final_model")
    model.save_pretrained(final_path)
    tokenizer.save_pretrained(final_path)
    print(f"âœ… Model saved to {final_path}")

if __name__ == "__main__":
    main()