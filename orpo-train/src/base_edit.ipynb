{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d64aa4cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import transformers\n",
    "from ast import literal_eval\n",
    "from trl import SFTTrainer, SFTConfig\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "from datasets import Dataset\n",
    "import json\n",
    "\n",
    "import pandas as pd\n",
    "import random\n",
    "import numpy as np\n",
    "import evaluate\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from tqdm import tqdm\n",
    "from peft import AutoPeftModelForCausalLM, LoraConfig\n",
    "\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1954d2ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "max_seq_length = 2048 # Choose any! We auto support RoPE Scaling internally!\n",
    "dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
    "load_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False.\n",
    "\n",
    "# 4bit pre quantized models we support for 4x faster downloading + no OOMs.\n",
    "fourbit_models = [\n",
    "    \"unsloth/Llama-3.3-70B-Instruct-bnb-4bit\" # NEW! Llama 3.3 70B!\n",
    "] # More models at https://huggingface.co/unsloth\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"unsloth/Meta-Llama-3.1-8B-Instruct-bnb-4bit\", # or choose \"unsloth/Llama-3.2-1B-Instruct\"\n",
    "    max_seq_length = max_seq_length,\n",
    "    dtype = dtype,\n",
    "    load_in_4bit = load_in_4bit,\n",
    "    # token = \"hf_...\", # use one if using gated models like meta-llama/Llama-2-7b-hf\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "003674b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from unsloth.chat_templates import get_chat_template\n",
    "from unsloth.chat_templates import train_on_responses_only\n",
    "tokenizer = get_chat_template(\n",
    "    tokenizer,\n",
    "    chat_template = \"llama-3.1\",\n",
    ")\n",
    "\n",
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e005426",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 난수 고정\n",
    "def set_seed(random_seed):\n",
    "    torch.manual_seed(random_seed)\n",
    "    torch.cuda.manual_seed(random_seed)\n",
    "    torch.cuda.manual_seed_all(random_seed)  # if use multi-GPU\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    np.random.seed(random_seed)\n",
    "    random.seed(random_seed)\n",
    "\n",
    "set_seed(42) # magic number :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "531d06cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the train dataset\n",
    "# TODO Train Data 경로 입력\n",
    "dataset = pd.read_csv('./data/train.csv') \n",
    "\n",
    "# Flatten the JSON dataset\n",
    "records = []\n",
    "for _, row in dataset.iterrows():\n",
    "    problems = literal_eval(row['problems'])\n",
    "    record = {\n",
    "        'id': row['id'],\n",
    "        'paragraph': row['paragraph'],\n",
    "        'question': problems['question'],\n",
    "        'choices': problems['choices'],\n",
    "        'answer': problems.get('answer', None),\n",
    "        \"question_plus\": problems.get('question_plus', None),\n",
    "    }\n",
    "    # Include 'question_plus' if it exists\n",
    "    if 'question_plus' in problems:\n",
    "        record['question_plus'] = problems['question_plus']\n",
    "    records.append(record)\n",
    "        \n",
    "# Convert to DataFrame\n",
    "df = pd.DataFrame(records)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65ea19fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r = 256,           # Choose any number > 0! Suggested 8, 16, 32, 64, 128\n",
    "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
    "    # target_modules = [ \"o_proj\",\n",
    "    #                   \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
    "    lora_alpha = 256,  # Best to choose alpha = rank or rank*2\n",
    "    lora_dropout = 0, # Supports any, but = 0 is optimized\n",
    "    bias = \"none\",    # Supports any, but = \"none\" is optimized\n",
    "    # [NEW] \"unsloth\" uses 30% less VRAM, fits 2x larger batch sizes!\n",
    "    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for very long context\n",
    "    random_state = 42,\n",
    "    use_rslora = False,   # We support rank stabilized LoRA\n",
    "    loftq_config = None,  # And LoftQ\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "155ba9dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Dataset.from_pandas(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31e6bcb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROMPT_NO_QUESTION_PLUS = \"\"\"지문:\n",
    "{paragraph}\n",
    "\n",
    "질문:\n",
    "{question}\n",
    "\n",
    "선택지:\n",
    "{choices}\n",
    "\n",
    "1, 2, 3, 4, 5 중에 하나를 정답으로 고르세요.\n",
    "정답:\"\"\"\n",
    "\n",
    "PROMPT_QUESTION_PLUS = \"\"\"지문:\n",
    "{paragraph}\n",
    "\n",
    "질문:\n",
    "{question}\n",
    "\n",
    "<보기>:\n",
    "{question_plus}\n",
    "\n",
    "선택지:\n",
    "{choices}\n",
    "\n",
    "1, 2, 3, 4, 5 중에 하나를 정답으로 고르세요.\n",
    "정답:\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46c8cd63",
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_dataset = []\n",
    "for i in range(len(dataset)):\n",
    "    choices_string = \"\\n\".join([f\"{idx + 1} - {choice}\" for idx, choice in enumerate(dataset[i][\"choices\"])])\n",
    "\n",
    "    # <보기>가 있을 때\n",
    "    if dataset[i][\"question_plus\"]:\n",
    "        user_message = PROMPT_QUESTION_PLUS.format(\n",
    "            paragraph=dataset[i][\"paragraph\"],\n",
    "            question=dataset[i][\"question\"],\n",
    "            question_plus=dataset[i][\"question_plus\"],\n",
    "            choices=choices_string,\n",
    "        )\n",
    "    # <보기>가 없을 때\n",
    "    else:\n",
    "        user_message = PROMPT_NO_QUESTION_PLUS.format(\n",
    "            paragraph=dataset[i][\"paragraph\"],\n",
    "            question=dataset[i][\"question\"],\n",
    "            choices=choices_string,\n",
    "        )\n",
    "\n",
    "    # chat message 형식으로 변환\n",
    "    processed_dataset.append(\n",
    "        {\n",
    "            \"id\": dataset[i][\"id\"],\n",
    "            \"messages\": [\n",
    "                {\"role\": \"system\", \"content\": \"지문을 읽고 질문의 답을 구하세요.\"},\n",
    "                {\"role\": \"user\", \"content\": user_message},\n",
    "                {\"role\": \"assistant\", \"content\": f\"{dataset[i]['answer']}\"}\n",
    "            ],\n",
    "            \"label\": dataset[i][\"answer\"],\n",
    "        }\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9717ba23",
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8b841b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_dataset = Dataset.from_pandas(pd.DataFrame(processed_dataset))\n",
    "processed_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ad520ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.chat_template"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "156aeef4",
   "metadata": {},
   "source": [
    "```\n",
    "{'id': 'generation-for-nlp-425',\n",
    " 'messages': [{'role': 'system', 'content': '지문을 읽고 질문의 답을 구하세요.'},\n",
    "  {'role': 'user',\n",
    "   'content': '지문:\\n상소하여 아뢰기를 , “신이 좌참 찬 송준길이 올린 차자를 보았는데 , 상복(喪服) 절차에 대하여 논한 것이 신과는 큰 차이가 있었습니다 . 장자를 위하여 3년을 입는 까닭은 위로 ‘정체(正體)’가 되기 때문이고 또 전 중(傳重: 조상의 제사나 가문의 법통을 전함)하기 때문입니다 . …(중략) … 무엇보다 중요한 것은 할아버지와 아버지의 뒤를 이은 ‘정체’이지, 꼭 첫째이기 때문에 참 최 3년 복을 입는 것은 아닙니다 .”라고 하였다 .－현종실록 －ㄱ.기 사환국으로 정권을 장악하였다 .ㄴ.인 조반정을 주도 하여 집권세력이 되었다 .ㄷ.정조 시기에 탕평 정치의 한 축을 이루었다 .ㄹ.이 이와 성혼의 문인을 중심으로 형성되었다.\\n\\n질문:\\n상소한 인물이 속한 붕당에 대한 설명으로 옳은 것만을 모두 고르면?\\n\\n선택지:\\n1 - ㄱ, ㄴ\\n2 - ㄱ, ㄷ\\n3 - ㄴ, ㄹ\\n4 - ㄷ, ㄹ\\n\\n1, 2, 3, 4, 5 중에 하나를 정답으로 고르세요.\\n정답:'},\n",
    "  {'role': 'assistant', 'content': '2'}],\n",
    " 'label': 2}\n",
    "```\n",
    "을 Llama-3 Chat Template에 맞게 파싱"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "832fcf21",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "def formatting_prompts_func(example):\n",
    "    output_texts = []\n",
    "    for i in range(len(example[\"messages\"])):\n",
    "        output_texts.append(\n",
    "            tokenizer.apply_chat_template(\n",
    "                example[\"messages\"][i],\n",
    "                tokenize=False,\n",
    "            )\n",
    "        )\n",
    "    return output_texts\n",
    "\n",
    "pprint(formatting_prompts_func(processed_dataset.select(range(1))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc6a8d81",
   "metadata": {},
   "source": [
    "일단 베이스라인 코드 대로면 우리는 llama 3의 챗 탬플릿인\n",
    "<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n'\n",
    " '\\n' 이후의 출력값만을 사용해야함 \n",
    "\n",
    "베이스 라인 코드에서는 [-2]로 뒤에서 두번째 즉 답변만 추출했음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9ac8c7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(element):\n",
    "    outputs = tokenizer(\n",
    "        formatting_prompts_func(element),\n",
    "        truncation=False,\n",
    "        padding=False,\n",
    "        return_overflowing_tokens=False,\n",
    "        return_length=False,\n",
    "    )\n",
    "    return {\n",
    "        \"input_ids\": outputs[\"input_ids\"],\n",
    "        \"attention_mask\": outputs[\"attention_mask\"],\n",
    "    }\n",
    "\n",
    "# 데이터 토큰화\n",
    "tokenized_dataset = processed_dataset.map(\n",
    "    tokenize,\n",
    "    remove_columns=list(processed_dataset.features),\n",
    "    batched=True,\n",
    "    num_proc=4,\n",
    "    load_from_cache_file=True,\n",
    "    desc=\"Tokenizing\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79e046bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_dataset = tokenized_dataset.train_test_split(test_size=0.25, seed=42)\n",
    "\n",
    "train_dataset = tokenized_dataset['train']\n",
    "eval_dataset = tokenized_dataset['test']\n",
    "# 데이터 확인\n",
    "print(tokenizer.decode(train_dataset[0][\"input_ids\"], skip_special_tokens=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35ec2669",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import evaluate\n",
    "\n",
    "# ---------------------------------------------------\n",
    "# 1. 메트릭 & 매핑 설정\n",
    "# ---------------------------------------------------\n",
    "acc_metric = evaluate.load(\"accuracy\")\n",
    "f1_metric = evaluate.load(\"f1\")\n",
    "candidate_labels = [\"1\", \"2\", \"3\", \"4\", \"5\"]\n",
    "int_output_map = {label: i for i, label in enumerate(candidate_labels)}\n",
    "\n",
    "# ---------------------------------------------------\n",
    "# 2. 전처리 (Logits -> Argmax)\n",
    "# ---------------------------------------------------\n",
    "def preprocess_logits_for_metrics(logits, labels):\n",
    "    if isinstance(logits, tuple): logits = logits[0]\n",
    "    return logits.argmax(dim=-1)\n",
    "\n",
    "# ---------------------------------------------------\n",
    "# 3. 메트릭 계산\n",
    "# ---------------------------------------------------\n",
    "def compute_metrics(eval_res):\n",
    "    predictions, labels = eval_res\n",
    "    \n",
    "    # Numpy 변환\n",
    "    if isinstance(predictions, torch.Tensor): predictions = predictions.cpu().numpy()\n",
    "    if isinstance(labels, torch.Tensor): labels = labels.cpu().numpy()\n",
    "\n",
    "    final_preds, final_refs = [], []\n",
    "\n",
    "    for i in range(len(labels)):\n",
    "        # -100(패딩)이 아닌 유효한 인덱스만 추출\n",
    "        valid_indices = np.where(labels[i] != -100)[0]\n",
    "        if len(valid_indices) == 0: continue\n",
    "            \n",
    "        # 정답 위치(target_idx) 찾기: 기본은 마지막, EOS면 그 앞\n",
    "        target_idx = valid_indices[-1]\n",
    "        if labels[i][target_idx] == tokenizer.eos_token_id and len(valid_indices) > 1:\n",
    "            target_idx = valid_indices[-2]\n",
    "            \n",
    "        # [핵심] 예측 위치(pred_idx)는 정답 위치보다 한 칸 앞(-1)\n",
    "        pred_idx = max(0, target_idx - 1)\n",
    "\n",
    "        # 값 추출 및 디코딩\n",
    "        decoded_label = tokenizer.decode([labels[i][target_idx]], skip_special_tokens=True).strip()\n",
    "        decoded_pred = tokenizer.decode([predictions[i][pred_idx]], skip_special_tokens=True).strip()\n",
    "        if i%10==0:\n",
    "            print(f\"Decoded Label: {decoded_label}, Decoded Pred: {decoded_pred}\")\n",
    "        # 매핑 후 리스트 추가 (매핑 실패 시 -1)\n",
    "        final_refs.append(int_output_map.get(decoded_label, -1))\n",
    "        final_preds.append(int_output_map.get(decoded_pred, -1))\n",
    "\n",
    "    return {\n",
    "        \"accuracy\": acc_metric.compute(predictions=final_preds, references=final_refs)[\"accuracy\"],\n",
    "        \"f1\": f1_metric.compute(predictions=final_preds, references=final_refs, average=\"macro\")[\"f1\"]\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b165211",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pad token 설정 -100 으로 마스킹한거\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "tokenizer.special_tokens_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed8f3a27",
   "metadata": {},
   "outputs": [],
   "source": [
    "from trl import SFTTrainer, SFTConfig\n",
    "from transformers import EarlyStoppingCallback\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    "    train_dataset = train_dataset,\n",
    "    eval_dataset = eval_dataset, # Can set up evaluation!\n",
    "            preprocess_logits_for_metrics=preprocess_logits_for_metrics,\n",
    "        compute_metrics=compute_metrics,\n",
    "    args = SFTConfig(\n",
    "        dataset_text_field = \"text\",\n",
    "        per_device_train_batch_size = 4,\n",
    "        per_device_eval_batch_size=4,\n",
    "        gradient_accumulation_steps = 8, # Use GA to mimic batch size!\n",
    "        warmup_ratio=0.1,\n",
    "        num_train_epochs = 3, # Set this for 1 full training run.\n",
    "        eval_steps=10,\n",
    "        metric_for_best_model=\"eval_f1\",\n",
    "        eval_strategy=\"steps\",\n",
    "        save_strategy=\"best\",\n",
    "        save_steps=10,\n",
    "        save_total_limit=1,\n",
    "        learning_rate = 2e-5, \n",
    "        logging_steps = 1,\n",
    "        optim = \"adamw_8bit\",\n",
    "        weight_decay = 0.01,\n",
    "        lr_scheduler_type = \"cosine\",\n",
    "        seed = 42,\n",
    "        load_best_model_at_end=True,\n",
    "        report_to = \"none\", # Use TrackIO/WandB etc\n",
    "        # completion_only_loss=True,\n",
    "    ),\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=2)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a1dca7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from unsloth.chat_templates import train_on_responses_only\n",
    "trainer = train_on_responses_only(\n",
    "    trainer,\n",
    "    instruction_part = \"<|start_header_id|>user<|end_header_id|>\\n\\n\",\n",
    "    response_part = \"<|start_header_id|>assistant<|end_header_id|>\\n\\n\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdcd466f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Show current memory stats\n",
    "gpu_stats = torch.cuda.get_device_properties(0)\n",
    "start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n",
    "print(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\n",
    "print(f\"{start_gpu_memory} GB of memory reserved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "821520bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from unsloth import unsloth_train\n",
    "trainer_stats = unsloth_train(trainer)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
