# Reasoning Distillation via Analogical Reasoning Transfer
<div align="center">

[![Python](https://img.shields.io/badge/Python-3.12+-blue.svg)](https://www.python.org/)
[![Transformers](https://img.shields.io/badge/Transformers-yellow.svg)](https://huggingface.co/docs/transformers/)
[![ORPO](https://img.shields.io/badge/Method-ORPO-green.svg)](https://arxiv.org/abs/2403.07691)
[![Hydra](https://img.shields.io/badge/Config-Hydra-89b4fa.svg)](https://hydra.cc/)
[![Unsloth](https://img.shields.io/badge/Training-Unsloth-f97316.svg)](https://github.com/unslothai/unsloth)

</div>

## Project Overview

ë³¸ í”„ë¡œì íŠ¸ëŠ” **Naver Boostcamp AI Tech 8ê¸°** ê³¼ì •ì˜ ì¼í™˜ìœ¼ë¡œ ìˆ˜í–‰ë˜ì—ˆë‹¤.
KMMLU, ìˆ˜ëŠ¥í˜• ë¬¸ì œ, KLUE MRC ë“± **ê³ ë‚œì´ë„ ë‹¤ë‹¨ê³„ ì¶”ë¡  ë¬¸ì œ**ì—ì„œ Qwen3 ì‹œë¦¬ì¦ˆ ëª¨ë¸ì˜ ì„±ëŠ¥ í•œê³„ë¥¼ ë¶„ì„í•˜ê³ ,
ì´ë¥¼ ê°œì„ í•˜ê¸° ìœ„í•œ ë°©ë²•ìœ¼ë¡œ **Reasoning Distillation via Analogical Reasoning Transfer**ë¥¼ ì œì•ˆí•œë‹¤.

ë³¸ ì ‘ê·¼ì€ ì •ë‹µì´ë‚˜ ì§€ì‹ ìì²´ë¥¼ ì¦ë¥˜í•˜ëŠ” ê²ƒì´ ì•„ë‹ˆë¼,
**ë¬¸ì œë¥¼ í•´ê²°í•˜ëŠ” ì¶”ë¡  ê²½ë¡œ(Reasoning Path)ë¥¼ ì •ë ¬(alignment)í•˜ëŠ” ê²ƒ**ì„ ëª©í‘œë¡œ í•œë‹¤.

---
## TL;DR

### 1) Key result
- 200 ORPO Preference Pairs â†’ Upstage ìˆ˜ëŠ¥(425ë¬¸í•­)ì—ì„œ **+14ë¬¸ì œ(â‰ˆ +3.27%p)** ì¶”ê°€ ì •ë‹µ  
- Qwen3â€‘14Bâ€‘ORPOê°€ **Qwen3â€‘30Bâ€‘A3Bâ€‘Instructì™€ ë™ê¸‰ ë˜ëŠ” ìš°ìˆ˜**í•œ ê³¼ëª©ë³„ ì„±ëŠ¥ ë‹¬ì„±

### 2) í•µì‹¬ ìš”ì•½
- ì…ë ¥: Teacher(Geminiâ€‘3â€‘Flash)ë¡œë¶€í„° ì„ ë³„í•œ **Hard Negatives** (200ê°œ)  
- ë°©ë²•: Teacherì˜ ì¶”ë¡  ê²½ë¡œ ì „ì´(Analogical Transfer) + ORPOë¡œ Decision Boundaryë§Œ êµ­ì†Œ ì •ë ¬ (SFT ë¶ˆí•„ìš”)  
- ì˜ë¯¸: ì†Œìˆ˜ì˜ Preference ë°ì´í„°ë¡œ ë„ë©”ì¸ì— ë”°ë¼ ì„±ëŠ¥ ê°œì„  ê°€ëŠ¥

## Repository Architecture

ë³¸ í”„ë¡œì íŠ¸ëŠ” ëª¨ë…¸ë ˆí¬ êµ¬ì¡°ë¡œ êµ¬ì„±ë˜ì–´ ìˆìœ¼ë©°, `feature/ver-3` ë¸Œëœì¹˜ê°€ ë©”ì¸ í—ˆë¸Œ ì—­í• ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.

| Module | ì—­í•  | README |
|--------|------|--------|
| **seed-data** | ì‹œë“œ ë¬¸í•­ ìˆ˜ì§‘ ë° Hard Negative ìƒì„± | [ìƒì„¸ ë¬¸ì„œ](./seed-data/README.md) |
| **preference-data** | Analogical Transfer ê¸°ë°˜ ë°ì´í„° ì¦ê°• | [ìƒì„¸ ë¬¸ì„œ](./preference-data/README.md) |
| **orpo-train** | ORPO í•™ìŠµ íŒŒì´í”„ë¼ì¸ (Hydra ì„¤ì •) | [ìƒì„¸ ë¬¸ì„œ](./orpo-train/README.md) |
| **eda** | train , testì…‹ EDA | [ìƒì„¸ ë¬¸ì„œ](./eda/README.md) |
---
## Why Reasoning Alignment?

### Observed Issue

Qwen3 ì‹œë¦¬ì¦ˆ ëª¨ë¸ì€ ë‹¨ì¼ ì‚¬ì‹¤(fact)ì— ëŒ€í•œ ì§ˆë¬¸ì—ì„œëŠ” ë†’ì€ ì •í™•ë„ë¥¼ ë³´ì´ì§€ë§Œ,
ì—¬ëŸ¬ ì •ë³´ë¥¼ ì—°ê²°í•´ì•¼ í•˜ëŠ” **ë³µí•© ì¸ê³¼Â·ë‹¨ê³„ì  ì¶”ë¡  ë¬¸ì œ**ì—ì„œëŠ” ì¼ê´€ë˜ê²Œ ì„±ëŠ¥ì´ ì €í•˜ëœë‹¤
(íŠ¹íˆ í•œêµ­ì‚¬, ì—­ì‚¬, ì‹¬ë¦¬, ê²½ì œ ê³„ì—´ ë¬¸ì œì—ì„œ ë‘ë“œëŸ¬ì§).

### Core Insight

2025ë…„ ê¸°ì¤€ LLMì€ ì´ë¯¸ ì¶©ë¶„í•œ ì§€ì‹ì„ ë³´ìœ í•˜ê³  ìˆë‹¤.
ë¬¸ì œëŠ” *ë¬´ì—‡ì„ ì•„ëŠ”ê°€*ê°€ ì•„ë‹ˆë¼, **ê·¸ ì§€ì‹ì„ ì–´ë–¤ ìˆœì„œì™€ ê¸°ì¤€ìœ¼ë¡œ ì—°ê²°í•˜ëŠ”ê°€**ì´ë‹¤.

> ì¦‰, ì„±ëŠ¥ ì €í•˜ì˜ ì›ì¸ì€ ì§€ì‹ ë¶€ì¡±ì´ ì•„ë‹ˆë¼
> **ì¶”ë¡  ê²½ë¡œ(Reasoning Path)ì˜ ì •ë ¬ ì‹¤íŒ¨**ì— ìˆë‹¤.

---

## Why Not SFT?

Supervised Fine-Tuning(SFT)ì€ ê°€ì¥ ì§ê´€ì ì¸ í•´ê²°ì±…ì´ì§€ë§Œ,
ì´ë¯¸ ê³ ë„ë¡œ í•™ìŠµëœ ëª¨ë¸ì—ëŠ” ë‹¤ìŒê³¼ ê°™ì€ ìœ„í—˜ì´ ì¡´ì¬í•œë‹¤.

* **Distribution Collapse**: ì†ŒëŸ‰ ë°ì´í„°ë¡œ ì¸í•œ ì¶œë ¥ ë¶„í¬ ì™œê³¡
* **Catastrophic Forgetting**: ê¸°ì¡´ ì¶”ë¡  ëŠ¥ë ¥ ì†ì‹¤
* Thinking / Non-Thinking í•˜ì´ë¸Œë¦¬ë“œ ëª¨ë¸ì—ì„œ ì¶”ë¡  í’ˆì§ˆ ì €í•˜

ë”°ë¼ì„œ ë³¸ í”„ë¡œì íŠ¸ëŠ” SFT ëŒ€ì‹ ,
**ê¸°ì¡´ ë¶„í¬ë¥¼ ìœ ì§€í•˜ë©´ì„œ ì¶”ë¡  ê²½ë¡œë§Œ ì„ íƒì ìœ¼ë¡œ ì •ë ¬í•˜ëŠ” ë°©ì‹**ì„ ì±„íƒí•œë‹¤.

---

## Our Approach: Preference-based Reasoning Alignment

ë³¸ ì—°êµ¬ëŠ” **ORPO(Odds Ratio Preference Optimization)** ê¸°ë°˜ Preference Learningì„ í™œìš©í•˜ì—¬:

* ëª¨ë¸ì˜ ê¸°ì¡´ ë¶„í¬ë¥¼ ë³´ì¡´í•˜ë©´ì„œ
* ì˜¬ë°”ë¥¸ ì¶”ë¡  ê²½ë¡œëŠ” ê°•í™”í•˜ê³ 
* ì˜ëª»ëœ ì¶”ë¡  ê²½ë¡œëŠ” ì–µì œí•œë‹¤

### Key Components

1. **Hard Negative Mining**
   TeacherëŠ” ì„±ê³µí–ˆìœ¼ë‚˜ StudentëŠ” ì‹¤íŒ¨í•œ ë¬¸ì œë§Œ ì„ íƒì ìœ¼ë¡œ ì‚¬ìš©

2. **Logical Structure Extraction**
   ì‹œë“œ ë¬¸í•­ì—ì„œ ë„ë©”ì¸ ë…ë¦½ì ì¸ ì¶”ë¡  êµ¬ì¡° ì¶”ì¶œ

3. **Analogical Reasoning Transfer**
   ì¶”ë¡  êµ¬ì¡°ë¥¼ ìƒˆë¡œìš´ ë„ë©”ì¸Â·ë§¥ë½ìœ¼ë¡œ ì „ì´

4. **ORPO-based Alignment**
   Chosen / Rejected ê°„ Odds Ratio ìµœì í™”

---

### Hard Negative Mining

![Hard Negative Mining Process](./asset/seed.png)

TeacherëŠ” ì„±ê³µí–ˆìœ¼ë‚˜ StudentëŠ” ì‹¤íŒ¨í•œ ë¬¸í•­(Hard Negative)ì„ ì§‘ì¤‘ì ìœ¼ë¡œ íƒ€ê²ŸíŒ…í•˜ì—¬:
1. Teacherì˜ ì„±ê³µì ì¸ ì¶”ë¡  ê²½ë¡œ(Golden Path)ì™€ Studentì˜ ì‹¤íŒ¨í•œ ì¶”ë¡  ê²½ë¡œ(Failed Path) ì¶”ì¶œ
2. ë„ë©”ì¸ ë…ë¦½ì ì¸ ë…¼ë¦¬ êµ¬ì¡° íŒ¨í„´ ë¶„ì„
3. ì¶”ì¶œëœ êµ¬ì¡°ë¥¼ ìƒˆë¡œìš´ ë„ë©”ì¸/ë§¥ë½ìœ¼ë¡œ ì „ì´í•˜ì—¬ ì¦ê°• ë°ì´í„° ìƒì„±
4. ORPOë¡œ Student ëª¨ë¸ í•™ìŠµ

### Blueprint Prompting Strategy
![alt text](./asset/work.png)
Teacherì—ê²Œ ë‹¨ìˆœíˆ â€œë¹„ìŠ·í•œ ë¬¸ì œë¥¼ ìƒì„±í•˜ë¼â€ê³  ìš”ì²­í•˜ì§€ ì•ŠëŠ”ë‹¤.
ëŒ€ì‹ , **ì„±ê³µí•œ ì¶”ë¡  ê²½ë¡œì™€ ì‹¤íŒ¨í•œ ì¶”ë¡  ê²½ë¡œë¥¼ í•¨ê»˜ ì œê³µ**í•˜ëŠ”
**Blueprint Prompting** ì „ëµì„ ì‚¬ìš©í•œë‹¤.

**Input**

* Golden Reasoning Path (Teacherì˜ ì„±ê³µ ì¶”ë¡ )
* Rejected Reasoning Path (Studentì˜ ì‹¤íŒ¨ ì¶”ë¡ )
* Transfer Target (ìƒˆ ë„ë©”ì¸/ë§¥ë½)

**Output**

* ë™ì¼í•œ ë…¼ë¦¬ êµ¬ì¡°ë¥¼ ìœ ì§€í•œ ìƒˆë¡œìš´ ë¬¸ì œ
* ëª…í™•íˆ êµ¬ë¶„ëœ Chosen / Rejected ì‘ë‹µ ìŒ

ì´ ë°©ì‹ì€ ì •ë‹µ ì—¬ë¶€ê°€ ì•„ë‹ˆë¼
**ì¶”ë¡  ê²½ë¡œì˜ ì •ë ¬ ì—¬ë¶€ë¥¼ ë°ì´í„° í’ˆì§ˆ ê¸°ì¤€ìœ¼ë¡œ ì‚¼ëŠ”ë‹¤.**

---
#### ì¦ê°• ë°ì´í„° ì˜ˆì‹œ

**ì¶”ì¶œëœ ë…¼ë¦¬ êµ¬ì¡°**: `ì •ì±… ëª©ì  â†’ êµ¬ì²´ì  ìˆ˜ë‹¨` ê°„ì˜ **ì¸ê³¼ì„± ì¶”ë¡ **

<table>
<tr>
<td width="50%">

#### Original Seed
**í¥ì„ ëŒ€ì›êµ°ì˜ ì„œì› ì² í ì •ì±…**

```
ë¬¸ì œ: ì²œì—¬ ê³³ì˜ ì„œì›ì„ ì² íí•˜ê³ ... 
      (ê°€) ì¸ë¬¼ì´ ì¶”ì§„í•œ ì •ì±…ìœ¼ë¡œ 
      ì˜³ì§€ ì•Šì€ ê²ƒì€?

ì„ ì§€:
1. ê²½ë³µê¶ì„ ì¤‘ê±´í•˜ì˜€ë‹¤
2. í˜¸í¬ì œë¥¼ ì‹¤ì‹œí•˜ì˜€ë‹¤
3. ë¹„ë³€ì‚¬ë¥¼ ê°•í™”í•˜ì˜€ë‹¤ [ì˜¤ë‹µ]
4. í†µë¦¬ê¸°ë¬´ì•„ë¬¸ì„ ì„¤ì¹˜í•˜ì˜€ë‹¤
```

**Chosen Reasoning (Teacher)**
```
1. í•µì‹¬ ëª©ì : ì™•ê¶Œ ê°•í™”
2. ìˆ˜ë‹¨ ë¶„ì„:
   - ì„œì› ì² í â†’ ì§€ë°© ì‚¬ë¦¼ ì„¸ë ¥ ì•½í™”
   - ë¹„ë³€ì‚¬ëŠ” ì‹ í•˜ ê¶Œë ¥ ê°•í™” ê¸°êµ¬
3. ë…¼ë¦¬: ì™•ê¶Œ ê°•í™” ì •ì±…ì—ì„œ
   ë¹„ë³€ì‚¬ ê°•í™”ëŠ” ëª¨ìˆœ
â†’ ë‹µ: 3ë²ˆ
```

**Rejected Reasoning (Student)**
```
1. ì„œì› ì² í = ìœ êµ ê°œí˜
2. ì •ì¡°ë„ ìœ êµ ê°œí˜ ì¶”ì§„
3. 4ë²ˆ í†µë¦¬ê¸°ë¬´ì•„ë¬¸ì€ ê·¼ëŒ€ ê¸°êµ¬
4. ì˜¤íŒ: ì •ì¡°ì˜ ì •ì±…ìœ¼ë¡œ ì°©ê°
â†’ ì˜¤ë‹µ: 4ë²ˆ
```

</td>
<td width="50%">

#### Augmented Data
**ê´‘ì¢…ì˜ ë…¸ë¹„ì•ˆê²€ë²•**

```
ë¬¸ì œ: ë…¸ë¹„ë“¤ì„ ì¡°ì‚¬í•˜ì—¬ ì›ë˜ 
      ì–‘ì¸ì´ì—ˆë˜ ìë“¤ì€ ëª¨ë‘ í•´ë°©...
      ì´ êµ­ì™•ì— ëŒ€í•œ ì„¤ëª…ì€?

ì„ ì§€:
1. ê³¼ê±°ì œë¥¼ ì‹¤ì‹œí•˜ì˜€ë‹¤ [ì •ë‹µ]
2. ì§€ë°©ì— 12ëª©ì„ ì„¤ì¹˜í•˜ì˜€ë‹¤
3. ë…ì„œì‚¼í’ˆê³¼ë¥¼ ì‹¤ì‹œí•˜ì˜€ë‹¤
4. ì²­í•´ì§„ì„ ì„¤ì¹˜í•˜ì˜€ë‹¤
```

**Chosen Reasoning (Aligned)**
```
1. í•µì‹¬ ëª©ì : í˜¸ì¡± ì„¸ë ¥ ê²¬ì œ
2. ìˆ˜ë‹¨ ë¶„ì„:
   - ë…¸ë¹„ì•ˆê²€ë²• â†’ í˜¸ì¡± ê²½ì œë ¥ ì•½í™”
   - ê³¼ê±°ì œ â†’ ì™•ê¶Œ ì§ì† ê´€ë£Œ ì–‘ì„±
3. ë…¼ë¦¬: í˜¸ì¡± ê²¬ì œ ì •ì±…ìœ¼ë¡œ
   ê³¼ê±°ì œê°€ ì¼ì¹˜
â†’ ë‹µ: 1ë²ˆ
```

**Rejected Reasoning (Simulated)**
```
1. ë…¸ë¹„ í•´ë°© = ì§€ë°© ì œë„ ê°œí˜
2. 12ëª© ì„¤ì¹˜ë„ ì§€ë°© ì œë„ ì •ë¹„
3. ì„±ì¢…ê³¼ ê´‘ì¢… ì •ì±… í˜¼ë™
4. ì˜¤íŒ: ìœ ì‚¬ ì‹œëŒ€ ì¸ë¬¼ ì°©ê°
â†’ ì˜¤ë‹µ: 2ë²ˆ
```

</td>
</tr>
</table>

**ì „ì´ í¬ì¸íŠ¸ ë¶„ì„**

- **ë…¼ë¦¬ êµ¬ì¡° ë³´ì¡´**: "ì •ì±… ëª©ì  â†’ êµ¬ì²´ì  ìˆ˜ë‹¨"ì˜ ì¸ê³¼ì„±ì„ ê²€ì¦í•˜ëŠ” ì¶”ë¡  íŒ¨í„´ì´ ë™ì¼í•˜ê²Œ ìœ ì§€ëœë‹¤.
- **ì˜¤ë¥˜ íŒ¨í„´ ì¬í˜„**: Studentê°€ "ìœ ì‚¬ ì‹œëŒ€ì˜ ë‹¤ë¥¸ ì¸ë¬¼"ë¡œ í˜¼ë™í•˜ëŠ” ì‹¤ìˆ˜ë¥¼ ìƒˆë¡œìš´ ë¬¸ì œì—ì„œë„ ì¬í˜„í•œë‹¤.
- **ë‚œì´ë„ ì¼ê´€ì„±**: ë°°ê²½ì§€ì‹ê³¼ í…ìŠ¤íŠ¸ ì¶”ë¡ ì˜ ê· í˜•ì´ ì›ë³¸ê³¼ ë™ì¼í•˜ê²Œ ìœ ì§€ëœë‹¤.
- **ë„ë©”ì¸ ì „ì´**: ì¡°ì„  í›„ê¸° â†’ ê³ ë ¤ ì‹œëŒ€ë¡œ ì‹œëŒ€ëŠ” ë°”ë€Œì—ˆì§€ë§Œ, "ì™•ê¶Œ ê°•í™”"ë¼ëŠ” ì •ì±… ëª©ì ì˜ ë…¼ë¦¬ëŠ” ë™ì¼í•˜ë‹¤.
---
## Performance

### Evaluation Setup

**Models**
- Baseline (14B): Qwen3-14B-AWQ
- Baseline (30B): Qwen3-30B-A3B-f8
- Target (ORPO): Qwen3-14B-bnb-4bit-ORPO

**Dataset**
- Upstage ìˆ˜ëŠ¥í˜• ë¬¸ì œ 869 ë¬¸ì œ (test set)
- ì´ 869ë¬¸í•­ ì¤‘ `425ê°œ`ì˜ `ìˆ˜ëŠ¥ ê¸°ì¶œë¬¸ì œ`ë§Œ ì‚¬ìš©
- ì •ë‹µ ë¼ë²¨: Gemini-3-Flash + GPT-5.2 ê° 3íšŒ ì¶”ë¡  í›„ ë‹¤ìˆ˜ê²°

**Key Result**

200ê°œì˜ ORPO í•™ìŠµ ë°ì´í„°ë§Œìœ¼ë¡œ **ì•½ 2ë°° í° 30B ëª¨ë¸ê³¼ ë™ê¸‰ ì´ìƒì˜ ì„±ëŠ¥** ë‹¬ì„±

---
![image.png](./asset/plot_two.png)

| ê³¼ëª© (Subject) | Qwen3-14b-AWQ | Qwen3-30b-A3b-Ins | Qwen3-14b-ORPO | ë¬¸í•­ ìˆ˜ (Support) |
| :--- | :---: | :---: | :---: | :---: |
| êµ­ì–´ | 71.43% | **73.55%** | `73.55%` | 189 |
| ìƒí™œê³¼ ìœ¤ë¦¬ | 71.67% | 71.67% | `75.00%` | 60 |
| ìœ¤ë¦¬ì™€ ì‚¬ìƒ | 70.77% | 67.69% | `73.85%` | 65 |
| ì‚¬íšŒë¬¸í™” | 58.33% | **68.75%** | 64.58% | 48 |
| ì •ì¹˜ì™€ ë²• | 57.14% | **60.32%** | 58.73% | 63 |

> ì‹¤ì œ ëŒ€íšŒ test setì˜ ê²½ìš° orpo ì ìš© í›„ f1-scoreê°€ ì†Œí­ í•˜ë½ í•˜ì˜€ìœ¼ë‚˜ ì•ì„œ ê°œì„ í•˜ê³ ì í–ˆë˜ ìœ í˜•ì¸ 
> test setì— ìˆëŠ” ì‹¤ì œ ìˆ˜ëŠ¥ ê¸°ì¶œë¬¸ì œ 425ë¬¸í•­ì„ ê¸°ì¤€ìœ¼ë¡œ í•˜ë©´ ì„±ëŠ¥ì´ ìœ ì˜ë¯¸í•˜ê²Œ í–¥ìƒë¨ì„ í™•ì¸í–ˆë‹¤.
> **ë‹¨ 200ê°œì˜ ë°ì´í„°ë¡œë„ ë„ë©”ì¸ ë§ì¶¤ reasoning alignmentê°€ ê°€ëŠ¥í•¨**ì„ í™•ì¸í–ˆë‹¤.

---
### Data Distribution Analysis [Umap Visualization]
![alt text](./asset/output.png)
ì‹œê°í™” ê²°ê³¼, UMAP ì‹œê°í™”ì—ì„œ Hard Training Dataê°€ ìˆ˜ëŠ¥ ê¸°ì¶œë¬¸í•­ê³¼ ë†’ì€ ì¤‘ì²©ì„ ë³´ì˜€ë‹¤.

ì¦‰, Hard Miningìœ¼ë¡œ êµ¬ì¶•í•œ ë°ì´í„°ê°€ ìˆ˜ëŠ¥ ë¬¸í•­ì˜ ì ì¬ì  ì¶”ë¡  ê³µê°„(Latent Space)ê³¼ ë§¤ìš° ìœ ì‚¬í–ˆê¸° ë•Œë¬¸ì—, ë‹¨ 200ê°œì˜ Preference Dataset ë§Œìœ¼ë¡œë„ ê°•ë ¥í•œ ë„ë©”ì¸ ì •ë ¬ì´ ê°€ëŠ¥í–ˆë˜ ê²ƒìœ¼ë¡œ ë³´ì¸ë‹¤.

## Key Findings

* Hard Negativeë§Œ í•™ìŠµí•˜ëŠ” ê²ƒì´ ì „ì²´ ë°ì´í„°ë³´ë‹¤ íš¨ìœ¨ì 
* ë…¼ë¦¬ êµ¬ì¡°ë§Œ ë³´ì¡´í•˜ë©´ ë„ë©”ì¸ì´ ë‹¬ë¼ë„ ì¶”ë¡  íŒ¨í„´ ì „ì´ ê°€ëŠ¥
* Blueprint Promptingì´ ë°ì´í„° í’ˆì§ˆì„ ê²°ì •ì ìœ¼ë¡œ ê°œì„ 
* ORPOëŠ” reasoning alignmentì— ì•ˆì •ì ìœ¼ë¡œ ì‘ë™

---

## Technical Stack

```yaml
Models:
  Teacher: Gemini-3-Flash
  Student Base: Qwen3-235B
  Student Target: Qwen3-14B

Training:
  Algorithm: ORPO (Odds Ratio Preference Optimization)
  Framework: Unsloth + ğŸ¤— Transformers + bitsandbytes (bnb-4bit)
  Hardware: V100 32GB Ã— 1
  Training Time: ~4 hours

Data:
  Source: KSAT-style questions (Upstage)
  Seed: ~200 Hard Negatives (Teacher âœ“ / Student âœ—)
  Augmentation: Blueprint Prompting + Analogical Transfer

Evaluation:
  Metrics: Accuracy, F1, Subject-wise analysis
```
---
## References
- [ORPO: Monolithic Preference Optimization](https://arxiv.org/abs/2403.07691)
- [Qwen Technical Report](https://arxiv.org/abs/2309.16609)
- [Chain-of-Thought Prompting](https://arxiv.org/abs/2201.11903)
- [Self-Rewarding Language Models](https://arxiv.org/abs/2401.10020)
- [Analogical Reasoning in Large Language Models](https://arxiv.org/abs/2310.01714)
